{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be79ebe",
   "metadata": {},
   "source": [
    "# CUR matrix decomposition for constructing PaiNN model\n",
    "This code introduces how to use CUR matrix decomposition for active learning. We use <a href=https://aip.scitation.org/doi/10.1063/1.3553717>symmetry function</a> as the fingerprints of atoms and select most representative atomic environments by using CUR. The most important images (contain important atoms) will be used for labelling.\n",
    "\n",
    "Note that the procedures can also used for any other model systems and different fingerprints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deeb336",
   "metadata": {},
   "source": [
    "## Generate symmetry functions (fingerprints of atomic environments)\n",
    "Firstly, we need several helpler functions to calculate the values of symmetry functions. Several symmetry functions are created, namely G2, G3, G4, and G5 functions (see definitions in <a href=https://compphysvienna.github.io/n2p2/>n2p2</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a834ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from torch_scatter import scatter_min\n",
    "\n",
    "def pair_cutoff(pair_dist, R_c, cutoff_type=2):\n",
    "    if cutoff_type == 1:\n",
    "        fc = 0.5 * (torch.cos(pair_dist * torch.pi / R_c) + 1)\n",
    "    elif cutoff_type == 2:\n",
    "        fc = torch.tanh(1 - pair_dist / R_c) ** 3\n",
    "    \n",
    "    return fc\n",
    "\n",
    "def G2_SF(tensors, j_elem, eta, R_s, R_c):\n",
    "    eta = eta.to(device=tensors['n_diff'].device)\n",
    "    R_s = R_s.to(device=tensors['n_diff'].device)\n",
    "    R_c = R_c.to(device=tensors['n_diff'].device)\n",
    "    pair_i_idx = tensors['atom_i_idx']\n",
    "    num_atoms = tensors['image_idx'].shape[0]\n",
    "        \n",
    "    # find the right neighbors\n",
    "    j_mask = (tensors['j_elems'] == j_elem)\n",
    "    sfs = torch.zeros((num_atoms, R_c.shape[0]), device=tensors['n_diff'].device) \n",
    "    \n",
    "    # calculate symmetry function values\n",
    "    pair_dist = tensors['n_dist'][j_mask].unsqueeze(dim=-1)\n",
    "#    pair_fc = 0.5 * (torch.cos(pair_dist * torch.pi / R_c) + 1)\n",
    "    pair_fc = pair_cutoff(pair_dist, R_c, cutoff_type=2)\n",
    "    pair_sfs = torch.exp(-eta * (pair_dist - R_s) ** 2) * pair_fc\n",
    "    sfs.index_add_(0, pair_i_idx[j_mask], pair_sfs)\n",
    "    \n",
    "    return sfs\n",
    "\n",
    "def G3_SF(tensors, j_elem, kappa, R_c):\n",
    "    kappa = kappa.to(device=tensors['n_diff'].device)\n",
    "    R_c = R_c.to(device=tensors['n_diff'].device)\n",
    "    pair_i_idx = tensors['atom_i_idx']\n",
    "    num_atoms = tensors['image_idx'].shape[0]\n",
    "        \n",
    "    # find the right neighbors\n",
    "    j_mask = (tensors['j_elems'] == j_elem)\n",
    "    sfs = torch.zeros((num_atoms, R_c.shape[0]), device=tensors['n_diff'].device) \n",
    "    \n",
    "    # calculate symmetry function values\n",
    "    pair_dist = tensors['n_dist'][j_mask].unsqueeze(dim=-1)\n",
    "#    pair_fc = 0.5 * (torch.cos(pair_dist * torch.pi / R_c) + 1)\n",
    "    pair_fc = pair_cutoff(pair_dist, R_c, cutoff_type=2)\n",
    "    pair_sfs = torch.cos(kappa * pair_dist) * pair_fc\n",
    "    sfs.index_add_(0, pair_i_idx[j_mask], pair_sfs)\n",
    "    \n",
    "    return sfs\n",
    "\n",
    "def G5_SF(tensors, j_elem, k_elem, zeta, Lambda, eta, R_c):\n",
    "    diff = tensors['n_diff']\n",
    "    dist = tensors['n_dist']   \n",
    "    pair_i_idx = tensors['atom_i_idx']\n",
    "    num_atoms = tensors['image_idx'].shape[0]\n",
    "    \n",
    "    zeta = zeta.to(device=diff.device)\n",
    "    Lambda = Lambda.to(device=diff.device)\n",
    "    eta = eta.to(device=diff.device)\n",
    "    R_c = R_c.to(device=diff.device)\n",
    "    \n",
    "    j_mask = (tensors['j_elems'] == j_elem)\n",
    "    k_mask = (tensors['j_elems'] == k_elem)\n",
    "    \n",
    "    # get relative index of neighbors\n",
    "    atom_idx_j_masked, j_inv_idx, j_counts = torch.unique_consecutive(\n",
    "        pair_i_idx[j_mask], return_inverse=True, return_counts=True,\n",
    "    )\n",
    "    atom_idx_k_masked, k_inv_idx, k_counts = torch.unique_consecutive(\n",
    "        pair_i_idx[k_mask], return_inverse=True, return_counts=True,\n",
    "    )\n",
    "    \n",
    "    g_idx = torch.arange(j_inv_idx.shape[0], device=diff.device)\n",
    "    idx_min, _ = scatter_min(g_idx, j_inv_idx)\n",
    "    j_ridx = g_idx - idx_min[j_inv_idx]\n",
    "\n",
    "    g_idx = torch.arange(k_inv_idx.shape[0], device=diff.device)\n",
    "    idx_min, _ = scatter_min(g_idx, k_inv_idx)\n",
    "    k_ridx = g_idx - idx_min[k_inv_idx]\n",
    "    \n",
    "    # get the matrix of M * N * ..., \n",
    "    # where M is the number of center atoms, \n",
    "    # N is the maximum number of their j or k neighbors\n",
    "    diff_ij = torch.zeros((num_atoms, j_counts.max(), 3), device=diff.device)\n",
    "    diff_ik = torch.zeros((num_atoms, k_counts.max(), 3), device=diff.device)\n",
    "    dist_ij = torch.zeros((num_atoms, j_counts.max()), device=diff.device)\n",
    "    dist_ik = torch.zeros((num_atoms, k_counts.max()), device=diff.device)\n",
    "    \n",
    "    diff_ij[pair_i_idx[j_mask], j_ridx] = diff[j_mask]\n",
    "    diff_ik[pair_i_idx[k_mask], k_ridx] = diff[k_mask]\n",
    "    dist_ij[pair_i_idx[j_mask], j_ridx] = dist[j_mask]\n",
    "    dist_ik[pair_i_idx[k_mask], k_ridx] = dist[k_mask]\n",
    "    \n",
    "    # calculate the values of different parts in angular symmetry functions\n",
    "    # Do remember to revise part 1！！！！！！！！！！ This implementaion will result in NaN gradient problem!!!\n",
    "    diff_ijk = torch.einsum(\"ijk, ilk -> ijl\", diff_ij, diff_ik)\n",
    "    dist_prod = (dist_ij.unsqueeze(dim=-1) * dist_ik.unsqueeze(dim=-2))\n",
    "    \n",
    "    # handling situation that j = k\n",
    "    if j_elem == k_elem:\n",
    "        dist_prod = torch.triu(dist_prod, diagonal = 1)   # this place is true\n",
    "        \n",
    "    idx_i, idx_j, idx_k = torch.where(dist_prod) \n",
    "\n",
    "    part_1 = diff_ijk[idx_i, idx_j, idx_k] / dist_prod[idx_i, idx_j, idx_k]\n",
    "    part_1 = (part_1.unsqueeze(dim=1) * Lambda + 1) ** zeta\n",
    "\n",
    "    part_2 = torch.exp(-eta * (dist_ij[idx_i, idx_j] ** 2 + dist_ik[idx_i, idx_k] ** 2).unsqueeze(dim=-1))\n",
    "\n",
    "    pair_fc_ij = pair_cutoff(dist_ij[idx_i, idx_j].unsqueeze(dim=-1), R_c, cutoff_type=2)\n",
    "    pair_fc_ik = pair_cutoff(dist_ik[idx_i, idx_k].unsqueeze(dim=-1), R_c, cutoff_type=2)\n",
    "    part_3 = pair_fc_ij * pair_fc_ik\n",
    "    sfs = torch.zeros((num_atoms, R_c.shape[0]), device=diff.device)\n",
    "    sfs = sfs.index_add(0, idx_i, part_1 * part_2 * part_3 * 2 ** (1-zeta))\n",
    "    \n",
    "    return sfs\n",
    "\n",
    "def G4_SF(tensors, j_elem, k_elem, zeta, Lambda, eta, R_c):\n",
    "    diff = tensors['n_diff']\n",
    "    dist = tensors['n_dist']   \n",
    "    pair_i_idx = tensors['atom_i_idx']\n",
    "    num_atoms = tensors['image_idx'].shape[0]\n",
    "    \n",
    "    zeta = zeta.to(device=diff.device)\n",
    "    Lambda = Lambda.to(device=diff.device)\n",
    "    eta = eta.to(device=diff.device)\n",
    "    R_c = R_c.to(device=diff.device)\n",
    "    \n",
    "    j_mask = (tensors['j_elems'] == j_elem)\n",
    "    k_mask = (tensors['j_elems'] == k_elem)\n",
    "    \n",
    "    # get relative index of neighbors\n",
    "    atom_idx_j_masked, j_inv_idx, j_counts = torch.unique_consecutive(\n",
    "        pair_i_idx[j_mask], return_inverse=True, return_counts=True,\n",
    "    )\n",
    "    atom_idx_k_masked, k_inv_idx, k_counts = torch.unique_consecutive(\n",
    "        pair_i_idx[k_mask], return_inverse=True, return_counts=True,\n",
    "    )\n",
    "    \n",
    "    g_idx = torch.arange(j_inv_idx.shape[0], device=diff.device)\n",
    "    idx_min, _ = scatter_min(g_idx, j_inv_idx)\n",
    "    j_ridx = g_idx - idx_min[j_inv_idx]\n",
    "\n",
    "    g_idx = torch.arange(k_inv_idx.shape[0], device=diff.device)\n",
    "    idx_min, _ = scatter_min(g_idx, k_inv_idx)\n",
    "    k_ridx = g_idx - idx_min[k_inv_idx]\n",
    "    \n",
    "    # get the matrix of M * N * ..., \n",
    "    # where M is the number of center atoms, \n",
    "    # N is the maximum number of their j or k neighbors\n",
    "    diff_ij = torch.zeros((num_atoms, j_counts.max(), 3), device=diff.device)\n",
    "    diff_ik = torch.zeros((num_atoms, k_counts.max(), 3), device=diff.device)\n",
    "    dist_ij = torch.zeros((num_atoms, j_counts.max()), device=diff.device)\n",
    "    dist_ik = torch.zeros((num_atoms, k_counts.max()), device=diff.device)\n",
    "    \n",
    "    diff_ij[pair_i_idx[j_mask], j_ridx] = diff[j_mask]\n",
    "    diff_ik[pair_i_idx[k_mask], k_ridx] = diff[k_mask]\n",
    "    dist_ij[pair_i_idx[j_mask], j_ridx] = dist[j_mask]\n",
    "    dist_ik[pair_i_idx[k_mask], k_ridx] = dist[k_mask]\n",
    "    \n",
    "    # calculate the values of different parts in angular symmetry functions\n",
    "    # Do remember to revise part 1！！！！！！！！！！ This implementaion will result in NaN gradient problem!!!\n",
    "    diff_ijk = torch.einsum(\"ijk, ilk -> ijl\", diff_ij, diff_ik)\n",
    "    dist_prod = (dist_ij.unsqueeze(dim=-1) * dist_ik.unsqueeze(dim=-2))\n",
    "    \n",
    "    # handling situation that j = k\n",
    "    if j_elem == k_elem:\n",
    "        dist_prod = torch.triu(dist_prod, diagonal = 1)   # this place is true\n",
    "        \n",
    "    idx_i, idx_j, idx_k = torch.where(dist_prod)\n",
    "    pair_dist_jk = torch.norm(diff_ik[idx_i, idx_k] - diff_ij[idx_i, idx_j], dim=-1)\n",
    "    jk_mask = pair_dist_jk.unsqueeze(-1) < R_c[0]\n",
    "    \n",
    "    part_1 = diff_ijk[idx_i, idx_j, idx_k] / dist_prod[idx_i, idx_j, idx_k]\n",
    "    part_1 = (part_1.unsqueeze(dim=1) * Lambda + 1) ** zeta\n",
    "\n",
    "    part_2 = torch.exp(-eta * (dist_ij[idx_i, idx_j] ** 2 + dist_ik[idx_i, idx_k] ** 2).unsqueeze(dim=-1))\n",
    "\n",
    "    pair_fc_ij = pair_cutoff(dist_ij[idx_i, idx_j].unsqueeze(dim=-1), R_c, cutoff_type=2)\n",
    "    pair_fc_ik = pair_cutoff(dist_ik[idx_i, idx_k].unsqueeze(dim=-1), R_c, cutoff_type=2)\n",
    "    pair_fc_jk = pair_cutoff(pair_dist_jk.unsqueeze(dim=-1), R_c, cutoff_type=2)\n",
    "    part_3 = pair_fc_ij * pair_fc_ik * pair_fc_jk\n",
    "\n",
    "    sfs = torch.zeros((num_atoms, R_c.shape[0]), device=diff.device)\n",
    "    sfs = sfs.index_add(0, idx_i[jk_mask], (part_1 * part_2 * part_3 * 2 ** (1-zeta))[jk_mask])\n",
    "    \n",
    "    return sfs\n",
    "\n",
    "bp_sf_fns = {'G2': G2_SF, 'G3': G3_SF, 'G4': G4_SF, 'G5': G5_SF}\n",
    "class BPSymmFunc:\n",
    "    \"\"\"\n",
    "    Get Behler-Parrinello style symmetry function values of atoms\n",
    "    \"\"\"\n",
    "    def __init__(self, sf_spec, compute_forces=True):\n",
    "        self.sf_spec = defaultdict(list)\n",
    "        self.compute_forces = compute_forces\n",
    "        for elem, elem_spec in sf_spec.items():\n",
    "            for spec in elem_spec:\n",
    "                fn = bp_sf_fns[spec['type']]\n",
    "                options = {k:torch.FloatTensor(v)\n",
    "                           if isinstance(v, list) else v \n",
    "                           for k, v in spec.items() if k != 'type'}\n",
    "                self.sf_spec[elem].append((fn, options))\n",
    "    \n",
    "    def __call__(self, tensors):        \n",
    "        # preprocess\n",
    "        num_atoms = tensors['num_atoms']\n",
    "        num_pairs = tensors['num_pairs']\n",
    "        \n",
    "        ## pair offset\n",
    "        pairs = tensors['pairs']\n",
    "        pair_offset = torch.cumsum(\n",
    "            torch.cat((torch.tensor([0], \n",
    "                                    device=num_atoms.device,\n",
    "                                    dtype=num_atoms.dtype,                                    \n",
    "                                   ), num_atoms[:-1])),\n",
    "            dim=0\n",
    "        )\n",
    "        pair_offset = torch.repeat_interleave(pair_offset, num_pairs)\n",
    "        pairs = pairs + pair_offset.unsqueeze(-1)\n",
    "        \n",
    "        ## get atom image index\n",
    "        image_idx = torch.arange(tensors['num_atoms'].shape[0],\n",
    "                                 device=pairs.device,\n",
    "                                )\n",
    "        image_idx = torch.repeat_interleave(image_idx, num_atoms)\n",
    "        \n",
    "        ## calculate distance\n",
    "        n_dist = torch.linalg.norm(tensors['n_diff'], dim=1)\n",
    "\n",
    "        fps = {}\n",
    "        for elem, elem_spec in self.sf_spec.items():    \n",
    "            sfs = []\n",
    "            i_elem = atomic_numbers[elem]\n",
    "            i_masked = self.get_i_masked(tensors, pairs, image_idx, n_dist, i_elem=i_elem)\n",
    "            for fn, options in elem_spec:\n",
    "                sf = fn(i_masked, **options)\n",
    "                sfs.append(sf)\n",
    "            sfs =  torch.hstack(sfs)\n",
    "            fps[elem] = {\n",
    "                'sfs': sfs.detach().cpu().numpy(),\n",
    "                'image_idx': i_masked['image_idx'].detach().cpu().numpy(),\n",
    "            }\n",
    "            \n",
    "        return fps\n",
    "    \n",
    "    def get_i_masked(self, tensors, pairs, image_idx, n_dist, i_elem):\n",
    "        \"\"\"This function aims to construct M * N matrices, \n",
    "        where M is the number of selected center atoms,\n",
    "        N is the maximum number of these atoms' neighbors.\n",
    "        \"\"\"        \n",
    "        i_mask = (tensors['elems'][pairs[:, 0]] == i_elem)\n",
    "        _, inverse_indices, counts = torch.unique_consecutive(pairs[:, 0][i_mask],\n",
    "                                                              return_inverse=True,\n",
    "                                                              return_counts=True)\n",
    "    \n",
    "        i_masked = {\n",
    "            'image_idx': image_idx[tensors['elems'] == i_elem],\n",
    "            'atom_i_idx': inverse_indices,                        # atom indices of i masked pairs in this batch           \n",
    "            'n_dist': n_dist[i_mask],                             # distances of i masked pairs\n",
    "            'n_diff': tensors['n_diff'][i_mask],                  # distance vectors of i masked pairs\n",
    "            'j_elems': tensors['elems'][pairs[:, 1]][i_mask],     # j elements of pairs\n",
    "        }\n",
    "        return i_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af1c48",
   "metadata": {},
   "source": [
    "We write a function to read the defined symmetry function parameters. The format of <font color = blue>input.nn</font> is the same to <a href=https://www.uni-goettingen.de/de/560580.html>RuNNer</a> and <a href=https://compphysvienna.github.io/n2p2/>n2p2</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6351423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ase.data import atomic_numbers\n",
    "\n",
    "n2p2_sf_type = {'2': 'G2', '3': 'G4', '9': 'G5'}\n",
    "def get_sf_dict(input_file = 'input.nn'):\n",
    "    \"\"\" Read symmetry functions parameters from input.nn \"\"\"\n",
    "    lines = []\n",
    "    for line in open(input_file, 'r'):\n",
    "        if line.startswith('symfunc'):\n",
    "            lines.append(line.strip())\n",
    "    sf_specs = {}\n",
    "    for line in lines:\n",
    "        line = line.split()\n",
    "        if re.match('[a-zA-Z]', line[4]):\n",
    "            try:\n",
    "                sf_specs[\" \".join(line[1:5])]\n",
    "            except:\n",
    "                sf_specs[\" \".join(line[1:5])] = {'eta': [], 'Lambda': [], 'zeta': [], 'R_c': []}\n",
    "            finally:\n",
    "                sf_specs[\" \".join(line[1:5])]['eta'].append(float(line[5]))\n",
    "                sf_specs[\" \".join(line[1:5])]['Lambda'].append(float(line[6]))\n",
    "                sf_specs[\" \".join(line[1:5])]['zeta'].append(float(line[7]))\n",
    "                sf_specs[\" \".join(line[1:5])]['R_c'].append(float(line[8]))\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                sf_specs[\" \".join(line[1:4])]\n",
    "            except:\n",
    "                sf_specs[\" \".join(line[1:4])] = {'eta': [], 'R_s': [], 'R_c': []}\n",
    "            finally:\n",
    "                sf_specs[\" \".join(line[1:4])]['eta'].append(float(line[4]))\n",
    "                sf_specs[\" \".join(line[1:4])]['R_s'].append(float(line[5]))\n",
    "                sf_specs[\" \".join(line[1:4])]['R_c'].append(float(line[6]))\n",
    "\n",
    "    new_sf_specs = {}\n",
    "    for k1, v1 in sf_specs.items():\n",
    "        sf_type = k1.split()\n",
    "        sf_dict = {'type': n2p2_sf_type[sf_type[1]], 'j_elem': atomic_numbers[sf_type[2]]}\n",
    "        sf_dict.update({k2: v2 for k2, v2 in v1.items()})\n",
    "        if len(sf_type) == 4:\n",
    "            sf_dict.update({'k_elem': atomic_numbers[sf_type[3]]})\n",
    "        try:\n",
    "            new_sf_specs[sf_type[0]]\n",
    "        except:\n",
    "            new_sf_specs[sf_type[0]] = []\n",
    "        finally:\n",
    "            new_sf_specs[sf_type[0]].append(sf_dict)\n",
    "            \n",
    "    return new_sf_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e08c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_specs = get_sf_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aead5f0",
   "metadata": {},
   "source": [
    "## Load dataset and calculate atomic importance\n",
    "\n",
    "We use a test trajectory to show how CUR works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31118f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PaiNN.data import AseDataset, collate_atomsdata\n",
    "dataset = AseDataset('../demo.traj', cutoff=6.0)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_atomsdata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "311c97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "symfunc = BPSymmFunc(sf_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73859770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect fingerpoints\n",
    "import numpy as np\n",
    "fps = {}\n",
    "batch_num = 0\n",
    "for device_batch in dataloader:\n",
    "    batch = {\n",
    "        k: v.to(device=device, non_blocking=True)\n",
    "        for (k, v) in device_batch.items()\n",
    "    }\n",
    "    batch_fps = symfunc(batch)\n",
    "    for elem in batch_fps.keys():\n",
    "        batch_fps[elem]['image_idx'] += batch_num\n",
    "        if fps.get(elem):\n",
    "            for k, v in batch_fps[elem].items():\n",
    "                fps[elem][k].append(v)\n",
    "        else:\n",
    "            fps[elem] = {k: [v] for k, v in batch_fps[elem].items()}\n",
    "\n",
    "    batch_num += len(batch['num_atoms'])\n",
    "\n",
    "for elem in fps.keys():\n",
    "    for k, v in fps[elem].items():        \n",
    "        fps[elem][k] = np.concatenate(v)\n",
    "        \n",
    "    # shuffle different atoms\n",
    "    p = np.random.permutation(len(fps[elem][k]))\n",
    "    fps[elem]['index'] = p\n",
    "    fps[elem]['sfs'] = fps[elem]['sfs'][p]\n",
    "    fps[elem]['image_idx'] = fps[elem]['image_idx'][p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92644f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from CUR import div_mat, CUR_decomposition_row\n",
    "from multiprocessing import Pool\n",
    "mat_num = 500\n",
    "# processors = os.cpu_count()\n",
    "processors = 10\n",
    "\n",
    "for elem in fps.keys():\n",
    "    split_sfs = div_mat(fps[elem]['sfs'], mat_num)\n",
    "    with Pool(processors) as pool:\n",
    "        W_mat, C_arr = [], []\n",
    "        for W, C in pool.map(CUR_decomposition_row, split_sfs):\n",
    "            W_mat.append(W)\n",
    "            C_arr.append(C)\n",
    "    W_mat = np.concatenate(W_mat, axis=1)\n",
    "    fps[elem]['atomic_importance'] = np.linalg.norm(W_mat, axis=0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5d951",
   "metadata": {},
   "source": [
    "## Add up atomic importance to get image importance\n",
    "By now we obtained the importance score for each atom. We then need to add up the importance score for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d01faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_selected = 100              # number of configurations we need to select from an iteration\n",
    "image_importance = np.zeros(len(dataset))\n",
    "\n",
    "for elem in fps.keys():\n",
    "    np.add.at(image_importance, fps[elem]['image_idx'], fps[elem]['atomic_importance'])\n",
    "selected_indices = np.argsort(image_importance)[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47c1e9",
   "metadata": {},
   "source": [
    "Then we can label the representative images by DFT after getting their indices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
